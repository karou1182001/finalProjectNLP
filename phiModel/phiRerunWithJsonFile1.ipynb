{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b884baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS FOR USING JUST ONE GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbdde5",
   "metadata": {},
   "source": [
    "Log to Hugging face to save finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20cdd87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b18d0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6983c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karou1182001\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6a20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09195e6c",
   "metadata": {},
   "source": [
    "Load your modified dataset from JSONL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae22477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 266 examples.\n"
     ]
    }
   ],
   "source": [
    "def load_modified_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)  # This expects a list of dicts\n",
    "    return data\n",
    "\n",
    "modified_data = load_modified_dataset(\"perfect_subset_zoroCOT_phi.json\")  # Replace with your actual file name\n",
    "print(f\"Loaded {len(modified_data)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a1a44",
   "metadata": {},
   "source": [
    "Convert the dataset into Hugging Face format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b42ad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/266 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 266/266 [00:00<00:00, 9518.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_for_training(examples):\n",
    "    return {\"text\": [ex[\"question\"] + \" \" + str(ex[\"gold_num\"]) for ex in examples]}\n",
    "\n",
    "train_dataset = Dataset.from_dict(format_for_training(modified_data))\n",
    "\n",
    "# Tokenize\n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#assign a padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca993ebf",
   "metadata": {},
   "source": [
    "Fine-tuning setup and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66f9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_phi2\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    torch_compile=False,\n",
    "    report_to=[],\n",
    "    run_name=\"gsm8k-finetune-phi\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f70cc9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_823902/2810881546.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 01:54, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.128300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=198, training_loss=1.6491286537863992, metrics={'train_runtime': 115.1934, 'train_samples_per_second': 6.927, 'train_steps_per_second': 1.719, 'total_flos': 794856605614080.0, 'train_loss': 1.6491286537863992, 'epoch': 2.962406015037594})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01065b",
   "metadata": {},
   "source": [
    "Save finetuned model locally and to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43319cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved locally.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"fine_tuned_phi\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_phi\")\n",
    "print(\"Model and tokenizer saved locally.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a9d9c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"karou1182001/fine-tuned-phi\")\n",
    "tokenizer.push_to_hub(\"karou1182001/fine-tuned-phi\")\n",
    "print(\"Model pushed to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eed7c2",
   "metadata": {},
   "source": [
    "Load model finetuned to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf3c501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 31.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (rotary_emb): PhiRotaryEmbedding()\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_phi\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_phi\")\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338c180",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe073877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model (zero-shot)...:   0%|          | 1/266 [00:00<03:25,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Example:\n",
      "Question: A skilled tailor is making a ceremonial robe for a grand festival. The robe requires 6 bolts of blue fiber and half that much white fiber. How many bolts in total does it take to complete the robe?\n",
      "Expected (gold): 9\n",
      "Predicted: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model (zero-shot)...: 100%|██████████| 266/266 [03:30<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correct predictions: 24/266\n",
      "Accuracy on dataset 1: 9.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "first_printed = False\n",
    "\n",
    "for item in tqdm(modified_data, desc=\"Evaluating fine-tuned model (zero-shot)...\"):\n",
    "    question = item[\"question\"].strip()\n",
    "    gold = str(item[\"gold_num\"])\n",
    "\n",
    "    prompt = question + \"\\nA:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    pred_numbers = re.findall(r\"\\d+\", decoded)\n",
    "    prediction = pred_numbers[-1] if pred_numbers else None\n",
    "\n",
    "    if not first_printed:\n",
    "        print(\"\\nFirst Example:\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected (gold): {gold}\")\n",
    "        print(f\"Predicted: {prediction}\")\n",
    "        first_printed = True\n",
    "\n",
    "    if prediction == gold:\n",
    "        correct += 1\n",
    "\n",
    "total = len(modified_data)\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"\\nCorrect predictions: {correct}/{total}\")\n",
    "print(f\"Accuracy on dataset 1: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

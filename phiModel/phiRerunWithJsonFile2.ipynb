{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ccbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS FOR USING JUST ONE GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef60047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9d1f3",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23dda31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 examples.\n"
     ]
    }
   ],
   "source": [
    "with open(\"perfect_subset_phi.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a445986",
   "metadata": {},
   "source": [
    "-Load finetune model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e840f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"karou1182001/fine-tuned-phi\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"karou1182001/fine-tuned-phi\")\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Model loaded from Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3acd9",
   "metadata": {},
   "source": [
    "Evaluate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7ce506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...:   0%|          | 1/200 [00:00<02:59,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Example:\n",
      "Question: There are some cats on the roof. Before the weaver begins assembling the garment, she carefully checks her inventory to avoid any surprises during production. A robe takes 10 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? After finding the result, she notes it down to order supplies for next week.\n",
      "Expected (gold): 15\n",
      "Predicted: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 200/200 [02:38<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correct predictions: 13/200\n",
      "Accuracy on dataset 2: 6.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "first_printed = False\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Evaluating...\"):\n",
    "    question = item[\"question\"].strip()\n",
    "    gold = str(item[\"gold_number\"])\n",
    "\n",
    "    # Zero-shot prompt \n",
    "    prompt = question + \"\\nA:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    pred_numbers = re.findall(r\"\\d+\", decoded)\n",
    "    prediction = pred_numbers[-1] if pred_numbers else None\n",
    "\n",
    "    if not first_printed:\n",
    "        print(\"\\nFirst Example:\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected (gold): {gold}\")\n",
    "        print(f\"Predicted: {prediction}\")\n",
    "        first_printed = True\n",
    "\n",
    "    if prediction == gold:\n",
    "        correct += 1\n",
    "\n",
    "total = len(dataset)\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"\\nCorrect predictions: {correct}/{total}\")\n",
    "print(f\"Accuracy on dataset 2: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
